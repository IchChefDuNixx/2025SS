{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f3a738-486b-45d6-8f18-0e4cda691378",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning in a Nutshell: Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fb85a-8672-4242-8f02-2f1c99ddbe5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A (given) Binary Classifier\n",
    "\n",
    "To begin, we assume the weights of a binary classifier are given as $w_1 = 1.0$ and $w_2 = 0.5$.\n",
    "In the second task we are going to learn the weights from data.\n",
    "\n",
    "We also assume, the feature extractor $\\phi(\\mathbf{x})$ is given as the identity function. For simplicity, we are working with the extracted feature vectors $[x_1, x_2]$ directly.\n",
    "\n",
    "### Our classifier\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\mathbf{w}(x) &= \\text{sign}(\\underbrace{[1, 0.5]}_{\\mathbf{w}} \\cdot \\underbrace{[x_1, x_2]}_{\\phi(x)})\\\\\n",
    "      &= \\text{sign}(\\underbrace{1}_{w_1} \\cdot x_1 + \\underbrace{0.5}_{w_2} \\cdot x_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### The sign-function maps from a score to a label\n",
    "$$\n",
    "\\begin{equation*}\n",
    "  \\text{sign}(z)=\\begin{cases}\n",
    "    +1, & \\text{if $z>0$}\\\\\n",
    "    -1, & \\text{if $z<0$}\\\\\n",
    "     0, & \\text{if $z = 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d5406",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Making predictions with the classifier $f$\n",
    "\n",
    "In order to make predictions, we need to implement $f$ in Python. This requires us to implement the individual pieces as python methods:\n",
    "- A `getScore` method that computes the **score**: $w_1 \\cdot x_1 + w_2 \\cdot x_2$\n",
    "- A `getLabel` method that computes the **label** from a given **score**: $\\text{sign}(\\cdot)$\n",
    "- A `predict` method that computes the **label** from two parameters $x_1$ and $x_2$ by calling the other methods.\n",
    "\n",
    "Define a class and implement the `getScore`, `getLabel` and `predict` methods.\n",
    "The constructor should expect two parameters $w_0$ and $w_1$ and store the values in instance variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e033369a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassifier:\n",
    "\n",
    "    def __init__(self, w1, w2):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "\n",
    "    def getScore(self, x1, x2):\n",
    "        return self.w1 * x1 + self.w2 * x2\n",
    "\n",
    "    def getLabel(self, z):\n",
    "        if z > 0:\n",
    "            return 1\n",
    "        elif z < 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def predict(self, x1, x2): \n",
    "        score = self.getScore(x1, x2)\n",
    "        return self.getLabel(score)\n",
    "\n",
    "    def getMargin(self, x1, x2, y):\n",
    "        score = self.getScore(x1, x2)\n",
    "        return score * y\n",
    "\n",
    "    def getHingeLoss(self, x1, x2, y, minMargin=1.0):\n",
    "        # gap = minMargin\n",
    "        return max(minMargin - self.getMargin(x1, x2, y), 0)\n",
    "\n",
    "    def verbosePrediction(self, x1, x2, y):\n",
    "        # print(f\"({x1}, {x2}) -> predicted label: {self.predict(x1, x2)} target label: {y} Margin: {self.getMargin(x1, x2, y)} HingeLoss: {self.getHingeLoss(x1, x2, y)}\")\n",
    "        print(f\"({x1}, {x2}) -> predicted label: {self.predict(x1, x2)} target label: {y} Margin: {self.getMargin(x1, x2, y)}, HingeLoss: {self.getHingeLoss(x1, x2, y)}\") \n",
    "\n",
    "    def train(self, x1, x2, y, learningRate=0.1):\n",
    "        loss = self.getHingeLoss(x1, x2, y)\n",
    "        if loss > 0:\n",
    "            # self.w1 += learningRate * y * x1\n",
    "            # self.w2 += learningRate * y * x2\n",
    "            self.w1 = self.w1 - learningRate * y * - x1\n",
    "            self.w2 = self.w2 - learningRate * y * - x2\n",
    "\n",
    "w1 = 1.0\n",
    "w2 = 0.5\n",
    "\n",
    "x1 = 2\n",
    "x2 = -2\n",
    "\n",
    "classifier = BinaryClassifier(w1, w2)\n",
    "# TODO\n",
    "classifier.predict(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3f388-ba94-447e-abd4-7b517a246714",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6222c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class BinaryClassifier:\n",
    "\n",
    "    def __init__(self, w1, w2, verbose=False):\n",
    "        self.w = np.array([w1, w2])\n",
    "        self.verbose=verbose\n",
    "\n",
    "    def getScore(self, point):\n",
    "        return self.w.dot(point)\n",
    "\n",
    "    def getLabel(self, z):\n",
    "        return np.sign(z)\n",
    "\n",
    "    def predict(self, point):\n",
    "        score = self.getScore(point)\n",
    "        return self.getLabel(score)\n",
    "\n",
    "    def getMargin(self, point, y):\n",
    "        score = self.getScore(point)\n",
    "        return score * y\n",
    "\n",
    "    def getHingeLoss(self, point, y, minMargin=1.0):\n",
    "        # gap = minMargin\n",
    "        return max(minMargin - self.getMargin(point, y), 0)\n",
    "\n",
    "    def verbosePrediction(self, point, y):\n",
    "        # print(f\"({x1}, {x2}) -> predicted label: {self.predict(x1, x2)} target label: {y} Margin: {self.getMargin(x1, x2, y)} HingeLoss: {self.getHingeLoss(x1, x2, y)}\")\n",
    "        print(f\"({point}) -> predicted label: {self.predict(point)} target label: {y} Margin: {self.getMargin(point, y)}, HingeLoss: {self.getHingeLoss(point, y)}\") \n",
    "\n",
    "    def train(self, point, y, learningRate=0.1):\n",
    "        loss = self.getHingeLoss(point, y)\n",
    "        if loss > 0:\n",
    "            grad = y * point\n",
    "            if self.verbose:\n",
    "                print(f'Grad {x}, weight update {learning_rate * grad}')\n",
    "            self.w = self.w - learningRate * grad\n",
    "\n",
    "    def train_batch(self, points, ys, learningRate=0.1):\n",
    "        gradients = []\n",
    "        running_loss = []\n",
    "        for p, y in zip(points, ys):\n",
    "            loss = self.getHingeLoss(p, y)\n",
    "            running_loss.append(loss)\n",
    "            if loss > 0: \n",
    "                grad = y * -1 * p\n",
    "            else:\n",
    "                grad = np.zeros(2)\n",
    "            if self.verbose:\n",
    "                print('for point ', p, y, loss, grad)\n",
    "\n",
    "            gradients.append(grad)\n",
    "        self.w = self.w - learningRate * np.mean(gradients, axis=0)\n",
    "\n",
    "        if self.verbose: \n",
    "            print(f'batch loss: {sum(running_loss)/len(running_loss)}')\n",
    "            print(gradients)\n",
    "            print(np.mean(gradients, axis=0))\n",
    "            print(f'new weights: {self.w}')\n",
    "\n",
    "w1 = 1.0\n",
    "w2 = 0.5\n",
    "\n",
    "x = np.array([2, -2])\n",
    "\n",
    "classifier = BinaryClassifier(w1, w2)\n",
    "# TODO\n",
    "classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25a9e0ac-00c3-4c2f-a3cb-3dda3e0bb434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: -1.0 label: 1\n",
      "prediction: 1.0 label: -1\n",
      "prediction: -1.0 label: -1\n",
      "for point  [1 2] 1 1.2 [-1 -2]\n",
      "for point  [2 0] -1 2.6 [2 0]\n",
      "for point  [-1  1] -1 0 [0. 0.]\n",
      "batch loss: 1.2666666666666666\n",
      "[array([-1, -2]), array([2, 0]), array([0., 0.])]\n",
      "[ 0.33333333 -0.66666667]\n",
      "new weights: [ 0.7 -0.3]\n",
      "for point  [1 2] 1 0.9 [-1 -2]\n",
      "for point  [2 0] -1 2.4000000000000004 [2 0]\n",
      "for point  [-1  1] -1 0.0 [0. 0.]\n",
      "batch loss: 1.1\n",
      "[array([-1, -2]), array([2, 0]), array([0., 0.])]\n",
      "[ 0.33333333 -0.66666667]\n",
      "new weights: [ 0.6 -0.1]\n",
      "1.0\n",
      "prediction: 1.0 label: 1\n",
      "prediction: 1.0 label: -1\n",
      "prediction: -1.0 label: -1\n"
     ]
    }
   ],
   "source": [
    "# Pruefung:\n",
    "w1 = 0.8\n",
    "w2 = -0.5\n",
    "eps = 0.3\n",
    "classifier = BinaryClassifier(w1, w2, verbose=True)\n",
    "# TODO\n",
    "data = [np.array(p) for p in [(1, 2), (2, 0), (-1, 1)]]\n",
    "labels = [1, -1, -1]\n",
    "for p,y in zip(data, labels):\n",
    "    print(f'prediction: {classifier.predict(p)} label: {y}')\n",
    "# round one:\n",
    "classifier.train_batch(data, labels, learningRate=eps)\n",
    "# round two:\n",
    "classifier.train_batch(data, labels, learningRate=eps)\n",
    "\n",
    "print(classifier.predict(x))\n",
    "\n",
    "for p,y in zip(data, labels):\n",
    "    print(f'prediction: {classifier.predict(p)} label: {y}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2924e9b-1f85-4488-b4eb-3921ef0deda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2,  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((2, 0)) * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe01b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Multiple predictions and the target labels\n",
    "\n",
    "Below you are given a list of training examples.\n",
    "Each training example comes with a target label.\n",
    "\n",
    "The target label is the label our classifier should predict, ... but most likely it doesn't since we did not train it yet.\n",
    "\n",
    "Let's first make the classifier predict a label for each training example and print the predicted label next to the target label such we can see when things go wrong.\n",
    "\n",
    "#### Implement printing as a method of `BinaryClassifier`, named `verbosePrediction` that expects $x_1$, $x_2$ and $y$ as parameters and prints, for each training example, the predicted label and the target label and if it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2080091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.5, 0.5)) -> predicted label: 1.0 target label: 1 Margin: 0.75, HingeLoss: 0.25\n",
      "((2, 0)) -> predicted label: 1.0 target label: 1 Margin: 2.0, HingeLoss: 0\n",
      "((-1, 1)) -> predicted label: -1.0 target label: 1 Margin: -0.5, HingeLoss: 1.5\n",
      "((1, -1)) -> predicted label: 1.0 target label: -1 Margin: -0.5, HingeLoss: 1.5\n",
      "((1, -2)) -> predicted label: 0.0 target label: -1 Margin: -0.0, HingeLoss: 1.0\n",
      "((-1, -1)) -> predicted label: -1.0 target label: -1 Margin: 1.5, HingeLoss: 0\n"
     ]
    }
   ],
   "source": [
    "data = [(0.5, 0.5), (2, 0), (-1, 1), (1, -1), (1, -2), (-1, -1)]\n",
    "labels = [1, 1, 1, -1, -1, -1]\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "    # print(classifier.predict(x[0], x[1]) == y)\n",
    "    # classifier.predict(x[0], x[1])\n",
    "    # classifier.verbosePrediction(x[0], x[1], y)\n",
    "    classifier.verbosePrediction(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9b44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Compute the margin\n",
    "\n",
    "Recall, the **score** of a training example is positive if the angle between the feature vector of the training example and the weight vector is acute. And the score is negative if the angle is obtuse.\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} = \\Vert \\mathbf{w} \\Vert \\cdot \\Vert \\mathbf{x} \\Vert \\cdot \\cos(\\omega)\n",
    "$$\n",
    "\n",
    "All points in feature space with a score = 0, constitute the decision boundary of the classifier.\n",
    "\n",
    "Now, the **margin** takes the score and the target label and quantifies the correctness of a prediction:\n",
    "$$\n",
    "\\text{margin}(\\mathbf{x}, y, \\mathbf{w}) = (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y\n",
    "$$\n",
    "\n",
    "The margin is positive if the score and the target label agree (regardless of which class it is). It is negative, if both quantities disagree.\n",
    "\n",
    "Add a method `getMargin` to your implementation and, additionally, print the margin for each training example in the `predictVerbose` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830159d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193cdef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Compute the Hinge-Loss of the classifier\n",
    "\n",
    "Based on the margin, we can now calculate the Hinge-Loss of the classifier.\n",
    "\n",
    "Like any other loss, the Hinge Loss is large if the classifier's prediction does **not** agree with the target label.\n",
    "This is the exact opposite of what the margin tells us.\n",
    "\n",
    "Thus, we can just use the negative margin to indicate disagreement between prediction and target label.\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{hinge}(x, y, \\mathbf{w}) = \\max \\{ \\text{gap} - \\underbrace{\\underbrace{(\\mathbf{w} \\cdot \\mathbf{x})}_{\\text{score}}y}_{\\text{margin}}, 0 \\}\n",
    "$$\n",
    "\n",
    "Intuitively, what is the role of $\\text{gap}$?\n",
    "- What if $gap = 0$?\n",
    "- What if $gap > 0$?\n",
    "\n",
    "Hint: Think about the closeness of training points to the decision boundary (Or rather the closeness of the decision boundary to the training points, since it's up to us to adjust the decision boundary based on the loss).\n",
    "![](linear-classifier-decision-boundary.png)\n",
    "\n",
    "Implement a method `getHingeLoss` that expects parameters $x_1$, $x_2$ and, additionally, a keyword parameter `gap` with a default value of $1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b643bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Implement the weight update\n",
    "\n",
    "Based on the hinge loss, we can construct a weight update rule and repeatedly update the weights.\n",
    "\n",
    "The goal of repeated weight updates is to adjust the decision boundary of the classifier, such that it classifies the training examples correctly.\n",
    "\n",
    "For each training example, we calculate the weight change $\\Delta \\mathbf{w}$ and then update the weights using $\\Delta \\mathbf{w}$. The update rule is given as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} - 0.1 \\cdot \\Delta \\mathbf{w}\n",
    "$$\n",
    "\n",
    "where the weight change is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w}  = \\begin{cases}\n",
    "    - [x_1 \\cdot y, x_2 \\cdot y], & \\qquad \\text{if} \\quad \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w}) > 0\\\\\n",
    "    0, & \\qquad \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: We obtain the weight change by calculating the partial derivatives of the loss with respect to each weight. Example for $w_1$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w})}{\\partial w_1} &= \\frac{\\partial}{\\partial w_1} (\\text{gap} - (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= \\frac{\\partial}{\\partial w_1} \\text{gap} - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot \\frac{\\partial}{\\partial (w_1 \\cdot x_1 + w_2 \\cdot x_2)} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y \\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot + w_2 \\cdot x_2)  \\cdot y\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot y)  \\cdot y\\\\\n",
    "   &= - x_1 \\cdot y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The derivates for $w_2$ are computed analogously.\n",
    "The weight change $\\Delta \\mathbf{w}$ is just the vector of these partial derivatives.\n",
    "\n",
    "#### Implement the update rule in a method `train`. This method expects $x_1$, $x_2$ and $y$ as parameters. You may also include an optional `eta` parameter for the learning rate, which is currently set to a fixed 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238d921",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 6: Train the classifier until it predicts all training points correctly\n",
    "\n",
    "Steps:\n",
    "- Loop over a number of epochs (e.g. 100)\n",
    "    - Initialize a variable `trainLoss = 0`\n",
    "    - Loop over all training examples\n",
    "        - Call `train` on each example\n",
    "        - Call `getHingeLoss` on each example and add the loss to `trainLoss`\n",
    "    - Print the average `trainLoss`\n",
    "- Verify that your classifier now makes correct predictions using the method `printStats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.7749999999999999, HingeLoss: 0.2250000000000001\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 2.05, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: -1.0 target label: 1 Margin: -0.3999999999999998, HingeLoss: 1.4\n",
      "(1, -1) -> predicted label: 1.0 target label: -1 Margin: -0.2999999999999997, HingeLoss: 1.2999999999999998\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 0.5750000000000004, HingeLoss: 0.4249999999999996\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.5999999999999999, HingeLoss: 0\n",
      "epoch 1 loss: 0.6374999999999998\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.825, HingeLoss: 0.17500000000000004\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.7999999999999996, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: -1.0 target label: 1 Margin: -0.0499999999999996, HingeLoss: 1.0499999999999996\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 0.05000000000000049, HingeLoss: 0.9499999999999995\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 1.1500000000000008, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.6999999999999997, HingeLoss: 0\n",
      "epoch 2 loss: 0.41666666666666646\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.875, HingeLoss: 0.125\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.5499999999999994, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 0.3000000000000005, HingeLoss: 0.6999999999999995\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 0.4000000000000006, HingeLoss: 0.5999999999999994\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 1.4750000000000008, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.7499999999999998, HingeLoss: 0\n",
      "epoch 3 loss: 0.27499999999999986\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.8999999999999999, HingeLoss: 0.10000000000000009\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.3999999999999992, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 0.5000000000000006, HingeLoss: 0.49999999999999944\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 0.6000000000000006, HingeLoss: 0.39999999999999936\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 1.8000000000000007, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.7999999999999998, HingeLoss: 0\n",
      "epoch 4 loss: 0.20416666666666652\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.9249999999999998, HingeLoss: 0.07500000000000018\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.2499999999999991, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 0.7000000000000006, HingeLoss: 0.2999999999999994\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 0.8000000000000007, HingeLoss: 0.1999999999999993\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.125000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.8499999999999996, HingeLoss: 0\n",
      "epoch 5 loss: 0.13333333333333316\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.9499999999999997, HingeLoss: 0.050000000000000266\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.099999999999999, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 0.9000000000000006, HingeLoss: 0.09999999999999942\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.450000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.8999999999999997, HingeLoss: 0\n",
      "epoch 6 loss: 0.06249999999999983\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 0.9749999999999999, HingeLoss: 0.025000000000000133\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.149999999999999, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.525000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 2.05, HingeLoss: 0\n",
      "epoch 7 loss: 0.033333333333333416\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 1.025, HingeLoss: 0\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.049999999999999, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.525000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 2.05, HingeLoss: 0\n",
      "epoch 8 loss: 0.0\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 1.025, HingeLoss: 0\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.049999999999999, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.525000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 2.05, HingeLoss: 0\n",
      "epoch 9 loss: 0.0\n",
      "(0.5, 0.5) -> predicted label: 1.0 target label: 1 Margin: 1.025, HingeLoss: 0\n",
      "(2, 0) -> predicted label: 1.0 target label: 1 Margin: 1.049999999999999, HingeLoss: 0\n",
      "(-1, 1) -> predicted label: 1.0 target label: 1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -1) -> predicted label: -1.0 target label: -1 Margin: 1.0000000000000007, HingeLoss: 0\n",
      "(1, -2) -> predicted label: -1.0 target label: -1 Margin: 2.525000000000001, HingeLoss: 0\n",
      "(-1, -1) -> predicted label: -1.0 target label: -1 Margin: 2.05, HingeLoss: 0\n",
      "epoch 10 loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "classifier = BinaryClassifier(w1, w2)\n",
    "n_epochs = 10\n",
    "eta = 0.05\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    trainLoss = 0.\n",
    "    for i in range(len(data)):\n",
    "        (x1, x2), y = data[i], labels[i]\n",
    "        trainLoss += classifier.getHingeLoss(x1, x2, y)\n",
    "        classifier.train(x1, x2, y, eta)\n",
    "        classifier.verbosePrediction(x1, x2, y)\n",
    "    print(f'epoch {epoch + 1} loss: {trainLoss / len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb4a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Congratulations! You have implemented the core machinery of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2212b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
